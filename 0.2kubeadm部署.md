- [linux环境下查看cpu内核数，内存条数，磁盘大小](https://blog.csdn.net/zhangliao613/article/details/79021606)

# 简介

kubeadm 是一个kubernetes集群引导启动命令。
# 安装kubeadm

- [安装kubeadm](https://kubernetes.io/docs/setup/)

## 安装运行时环境

从kubernetes1.14.0开始，kubeadm将尝试自动化的在Linux节点上查询容器的运行时间，通过扫描已知域套接字列表，
使用可检测的运行时环境和套接字可以从如下所示路径找到：

Docker的域套接字路径：/var/run/docker.sock
containerd的域套接字路径：/var/run/containerd/containerd.sock

## 安装kubeadm,kubelet以及kubectl

1.安装kubeadm,kubelet以及kubectl

kubelet:该组件需要安装在所有的集群节点之上，执行像启动pods和容器这样的操作。

kubectl:该组件是一个负责和你的集群通信的命令行cluster。

命令如下：
```
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
exclude=kube*
EOF

# Set SELinux in permissive mode (effectively disabling it)
setenforce 0
sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes

systemctl enable --now kubelet
```
由于Netfilter在默认情况下回启用网桥，所以为了避免通过网桥转发的IP数据包会被iptables规则过滤，导致传输数据包混乱，需要执行下面的命令：
```
cat /proc/sys/net/bridge/bridge-nf-call-ip6tables 
1
//如果是0的话就执行下面的命令。

cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

sysctl --system
```
如果想知道br_netfilter模块有没有被加载，可以执行命令lsmod | grep br_netfilte，查看是否有关于br_netfilter的信息。

2.在主节点上配置kubelet使用的cgroup驱动程序

当docker 正在使用的时候，kubeadm将自动的检查kubelet的cgroup驱动，然后在/var/lib/kubelet/kubeadm-flags.env文件做一些设置。
，如果使用不同CRI，你不得不更改这个文件/etc/default/kubelet 以及cgroup-driver值，如下：
```
KUBELET_EXTRA_ARGS=--cgroup-driver=<value>
```
kubeadm init和kubeadm join将使用该文件为kubelet获取额外的用户定义参数。

请注意，只有在CRI的cgroup驱动程序不是cgroupfs时才需要这样做，因为这已经是kubelet中的默认值。

重启kubelet:

```
systemctl daemon-reload
systemctl restart kubelet
```



kubeadm被设计成一个简单的方法为新用户开始尝试Kubernetes,可能第一次为现有用户来测试他们的应用程序,并将它们整合集群上容易,而且是一个构建块在其他生态系统和/或安装工具与一个更大的范围。

## 初始化master节点

主服务器是控制平面组件运行的机器，包括etcd(集群数据库)和API服务器(kubectl CLI与之通信)。

选择pod网络附加组件，并验证是否需要将任何参数传递给kubeadm初始化。根据您选择的第三方提供者的不同，您可能需要将 **--pd -network-cidr**设置为特定于提供者的值。

(可选操作)因为自1.14版以来，kubeadm将尝试使用一个已知域套接字路径列表来检测Linux上的容器运行时。
要使用不同的容器运行时，或者如果在提供的节点上安装了多个容器运行时，可以为 kubeadm init指定 **--crit -socket** 参数。

(可选操作)除非另有说明，否则kubeadm使用与默认网关相关联的网络接口来发布主IP。为了使用不同的网卡，在 kubeadm init 中可以简化指定参数 --apiserver-advertise-address=<ip-address>，为了部署，
为了部署IPv6 kubernertes clusters ,可以使用IPv6 地址，你必须指定一个地址，例如， **--apiserver-advertise-address=fd00::101**

(可选操作)在kubeadm初始化之前运行kubeadm配置映像，以验证与gcr.io的连接注册。

如果将具有不同体系结构的节点连接到集群，则在节点上为kube-proxy和kube-dns创建单独的部署或启动守护进程。这是因为这些组件的Docker映像目前不支持多架构。

要再次运行kubeadm init，必须首先拆除集群。

初始化检查：kubeadm init首先运行一系列预检查，以确保机器已经准备好运行Kubernetes。这些预检查公开警告并在出现错误时退出。然后下载并安装集群控制平面组件。这可能需要几分钟。

```
kubeadm init --kubernetes-version=v1.16.1 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12
```
输出结果：
```
[init] Using Kubernetes version: v1.16.1
[preflight] Running pre-flight checks
	[WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correctly
	[WARNING HTTPProxy]: Connection to "https://192.168.137.128" uses proxy "http://127.0.0.1:8118". If that is not intended, adjust your proxy settings
	[WARNING HTTPProxyCIDR]: connection to "10.96.0.0/12" uses proxy "http://127.0.0.1:8118". This may lead to malfunctional cluster setup. Make sure that Pod and Services IP ranges specified correctly as exceptions in proxy configuration
	[WARNING HTTPProxyCIDR]: connection to "10.244.0.0/16" uses proxy "http://127.0.0.1:8118". This may lead to malfunctional cluster setup. Make sure that Pod and Services IP ranges specified correctly as exceptions in proxy configuration
	[WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR Swap]: running with swap on is not supported. Please disable swap
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
```
让防护墙开启6443 10250端口
```
firewall-cmd --zone=public --add-port=10250/tcp --permanent
firewall-cmd --zone=public --add-port=6443/tcp --permanent
```
然后，重新载入
```
firewall-cmd --reload
```
如果无效果,就直接关闭防护墙：
```
systemctl stop firewalld
```
解决错误：ERROR Swap:

1. 需要/etc/sysconfig/kubelet
```
vim /etc/sysconfig/kubelet        #加入以下参数
//修改如下：
KUBELET_EXTRA_ARGS="--fail-swap-on=false"
```
2. 重新执行命令语句，（10月20日,推荐执行下面apiserver为master的地址）
```
#在初始化时加入--ignore选项
kubeadm init --kubernetes-version=v1.16.1  --apiserver-advertise-address=140.82.8.255 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --ignore-preflight-errors=Swap  
```
如果顺利执行命令，将下载并安装集群控制组件，这可能需要几分钟的时间。

如果出现如下错误，请看**报错2**：
```
[ERROR ImagePull]: failed to pull image k8s.gcr.io/kube-apiserver:v1.16.0: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
, error: exit status 1
```
如果能够成功，那么通过docker images可以看到所需的7个镜像文件。
```
[root@kubernetes-master ~]# docker images
REPOSITORY                           TAG                 IMAGE ID            CREATED             SIZE
k8s.gcr.io/kube-proxy                v1.16.1             20a2d7035165        5 weeks ago         82.1MB
k8s.gcr.io/kube-apiserver            v1.16.1             cfaa4ad74c37        5 weeks ago         210MB
k8s.gcr.io/kube-scheduler            v1.16.1             8931473d5bdb        5 weeks ago         81.6MB
k8s.gcr.io/kube-controller-manager   v1.16.1             efb3887b411d        5 weeks ago         158MB
quay.io/coreos/flannel               v0.11.0-amd64       ff281650a721        3 months ago        52.6MB
k8s.gcr.io/coredns                   1.3.1               eb516548c180        4 months ago        40.3MB
k8s.gcr.io/etcd                      3.3.10              2c4adeb21b4f        5 months ago        258MB
k8s.gcr.io/pause                     3.1                 da86e6ba6ca1        16 months ago       742kB
```
镜像下载好之后，重新执行该命令，用于生成我们需要的配置文件。
```
kubeadm init --kubernetes-version=v1.16.1 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --ignore-preflight-errors=Swap  
```
正确启动的输出结果如下：
```
.....
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 140.82.8.255:6443 --token ymikc5.550000o3f7s61onr \
    --discovery-token-ca-cert-hash sha256:abcd45806fdda7df11ef180cf65f96db44fdb9efa8a5f8893dd99730bf2ab2e6 

 

```
其中k8s.gcr.io/pause就是基础架构容器，可以不用启动，其他容器可以将它当成模板进行网络、存储卷复制。

特别注意：

其中有两个附件：CoreDNS和kube-proxy

CoreDNS：CoreDNS已经经历过三个版本：sky-dns（）----->kube-dns（1.3版本）----->CoreDNS（1.11版本）

kube-proxy：作为附件运行自托管与k8s之上，来帮忙负责生成service资源相关的iptables或者ipvs规则，在1.11版本默认使用ipvs。

**创建kube目录，添加kubectl配置**

如果需要使用非root用户运行kubectl，可以使用非root用户执行如下命令：
```
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```
如果不是在生产环境下，使用root用户，执行kubectl命令可以执行下面的命令，打开权限。
```
export KUBECONFIG=/etc/kubernetes/admin.conf
```
否则会爆出这样的错误：
```
[root@kubernetes-master ~]# kubectl get nodes
The connection to the server localhost:8080 was refused - did you specify the right host or port?
```
获取节点信息与获取组件状态，测试如下：
```
[yzw@kubernetes-master root]$ kubectl get nodes
NAME                STATUS     ROLES    AGE     VERSION
kubernetes-master   NotReady   master   8m40s   v1.16.1 
[yzw@kubernetes-master root]$ kubectl get componentstatus
NAME                 STATUS    MESSAGE             ERROR
scheduler            Healthy   ok                  
controller-manager   Healthy   ok                  
etcd-0               Healthy   {"health":"true"}
```

1.关闭集群
查看kubelet状态
```
systemctl status kubelet

journalctl -xeu kubelet
```
通过下面命令可以查看日志信息
```
tail /var/log/messages
```
在执行如上命令之后，你会看到如下错误，不过你可以继续进行下一步;
```
failed to load Kubelet config file /var/lib/kubelet/config.yaml, error failed to read kubelet config file "/var/lib/kubelet/config.yaml", error: open /var/lib/kubelet/config.yaml: no such file or directory
```
关闭kubelet:
```
systemctl stop kubelet
```
设置开启启动kubelet和docker
```
systemctl enable kubelet
systemctl enable docker
```

**部署Flannel覆盖网**

您必须安装一个pod网络附加组件，以便您的pod可以彼此通信。**必须在任何应用程序之前部署网络。此外，在网络安装之前，CoreDNS不会启动。kubeadm只支持基于容器网络接口(CNI)的网络(不支持kubenet)。**

> 请注意，Pod网络不能与任何主机网络重叠，因为这会导致问题。如果您发现您的网络插件首选的Pod网络与您的一些主机网络之间存在冲突，您应该考虑一个合适的CIDR替换，并在kubeadm init 和--pod-network-cidr期间使用它，
并在您的网络插件的YAML中作为替换。

要使Flannel正确工作，必须在kubeadm init中包含--pod-net-cidr=10.244.0.0/16。并且通过运行sysctl net.bridge将/proc/sys/net/bridge/bridge-nf-call-iptables设置为1。bridge-nf-call-iptables=1，将桥接的IPv4流量
传递给iptables的链。这是一些CNI插件工作的要求。

让防护墙开启8285和8247端口
```
firewall-cmd --zone=public --add-port=8285/tcp --permanent
firewall-cmd --zone=public --add-port=8247/tcp --permanent
```
然后，重新载入
```
firewall-cmd --reload
```
安装Flannel网络的命令：
```
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml
```
输出结果如下：
```
[yzw@kubernetes-master root]$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml
podsecuritypolicy.policy/psp.flannel.unprivileged created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds-amd64 created
daemonset.apps/kube-flannel-ds-arm64 created
daemonset.apps/kube-flannel-ds-arm created
daemonset.apps/kube-flannel-ds-ppc64le created
daemonset.apps/kube-flannel-ds-s390x created
```
其他测试：
```
#查看所有名称空间的pod，同时可以看到flannel已经正常启动
kubectl get pods --all-namespaces       

#查看名称空间为kube-system的pod
kubectl get pods -n kube-system         
```
输出结果如下：
```
[yzw@kubernetes-master root]$ kubectl get pods --all-namespaces 
NAMESPACE     NAME                            READY   STATUS    RESTARTS   AGE
kube-system   coredns-5644d7b6d9-2msqz        0/1     Running   0          34m
kube-system   coredns-5644d7b6d9-84bzp        0/1     Running   0          34m
kube-system   etcd-node1                      1/1     Running   0          33m
kube-system   kube-apiserver-node1            1/1     Running   0          33m
kube-system   kube-controller-manager-node1   1/1     Running   0          33m
kube-system   kube-flannel-ds-amd64-bd7g8     1/1     Running   0          49s
kube-system   kube-proxy-fczl9                1/1     Running   0          34m
kube-system   kube-scheduler-node1            1/1     Running   0          33m

[yzw@kubernetes-master root]$ kubectl get pods -n kube-system
NAME                            READY   STATUS    RESTARTS   AGE
coredns-5644d7b6d9-2msqz        0/1     Running   0          46m
coredns-5644d7b6d9-84bzp        0/1     Running   0          46m
etcd-node1                      1/1     Running   0          45m
kube-apiserver-node1            1/1     Running   0          45m
kube-controller-manager-node1   1/1     Running   0          45m
kube-flannel-ds-amd64-bd7g8     1/1     Running   0          12m
kube-proxy-fczl9                1/1     Running   0          46m
kube-scheduler-node1            1/1     Running   0          45m

```

## node节点环境的安装

1.首先在kubernetes-node1节点和kubernetes-node2节点安装docker,并执行开机自启动命令。

2.安装kubeadm和kubelet
```
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
exclude=kube*
EOF

# Set SELinux in permissive mode (effectively disabling it)
setenforce 0
sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

yum install -y kubelet kubeadm --disableexcludes=kubernetes

systemctl daemon-reload
systemctl enable --now kubelet
```
同样需要做的步骤：
```
cat /proc/sys/net/bridge/bridge-nf-call-ip6tables 
1
//如果是0的话就执行下面的命令。

cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

sysctl --system


vim /etc/sysconfig/kubelet        #加入以下参数
//修改如下：
KUBELET_EXTRA_ARGS="--fail-swap-on=false"
```
**将node2与node3,node4加入kubernetes集群** 

kubeadm init 输出的 kubeadm
join……命令，后面需要这个命令将各个节点加入集群中,补充说明，令牌（已经加密，生命周期24小时）用于
master 和加入的 node
之间相互身份之间验证，凭借这个令牌可以让任何人将认证的节点加入到该集群，如果需要对令牌进行增、删、查的操作，可以使用
kubeadm token 命令，具体可参看kubeadm token。
我因为在部署环境过程中，token超过了有效期24小时，所以需要创建新的token。使用命令：
```
//查看token
kubeadm token list
//创建token
kubeadm token create

//还需要一个加密串discovery-token-ca-cert-hash，可以通过一下命令获取：
openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | \
   openssl dgst -sha256 -hex | sed 's/^.* //'
```

修改kubeam join 后面的值

kubeadm join 140.82.8.255:6443 --token  zpiig7.n2gx3g0xd8yfyf4n --discovery-token-ca-cert-hash sha256:abcd45806fdda7df11ef180cf65f96db44fdb9efa8a5f8893dd99730bf2ab2e6 --ignore-preflight-errors=Swap

```
kubeadm join 140.82.8.255:6443 --token  dgknig.8ql0dxqu9tm0c6s1 --discovery-token-ca-cert-hash sha256:abcd45806fdda7df11ef180cf65f96db44fdb9efa8a5f8893dd99730bf2ab2e6 --ignore-preflight-errors=Swap
```
如果测试node1与node2是否有成功的加入集群，可以在master节点上使用如下命令查看：
```
kubectl get nodes
```
如果输出结果如下，说明节点已经成功加入集群：
```
[yzw@kubernetes-master root]$ kubectl get nodes
NAME    STATUS     ROLES    AGE     VERSION
node1   Ready    master   84m     v1.16.2
node2   Ready    <none>   11m     v1.16.2
node3   Ready    <none>   2m16s   v1.16.2
node4   Ready    <none>   10m     v1.16.2
```
在node2与node3,node4上安装网络附加组件，此处使用Flannel组件(此处说明一点，如果是有能连外网的话，在master节点使用kubectl apply命令就可以部署fabric了！)： 
```
docker pull quay.io/coreos/flannel:v0.11.0-amd64
```
拉取proxy与pause镜像
```
docker pull mirrorgooglecontainers/kube-proxy-amd64:v1.16.1 
docker pull mirrorgooglecontainers/pause:3.1

docker tag mirrorgooglecontainers/kube-proxy-amd64:v1.16.1 k8s.gcr.io/kube-proxy:v1.16.1 
docker tag mirrorgooglecontainers/pause:3.1 k8s.gcr.io/pause:3.1

docker rmi mirrorgooglecontainers/kube-proxy-amd64:v1.16.1 
docker rmi mirrorgooglecontainers/pause:3.1
```
下载完之后使用docker images命令查看下载好的镜像 
```
[root@kubernetes-node1 ~]# docker images
REPOSITORY               TAG                 IMAGE ID            CREATED             SIZE
k8s.gcr.io/kube-proxy    v1.16.1             20a2d7035165        5 weeks ago         82.1MB
quay.io/coreos/flannel   v0.11.0-amd64       ff281650a721        3 months ago        52.6MB
k8s.gcr.io/pause         3.1                 da86e6ba6ca1        16 months ago       742kB
```
通过在master节点使用kubectl get pods
--all-namespaces命令，可以测试集群中的CoreDNS
pod是否被安装且正在运行，你也可以使用kubectl get pods -n kube-system -o
wide命令
```
[yzw@kubernetes-master root]$ kubectl get pods -n kube-system -o wide
NAME                            READY   STATUS    RESTARTS   AGE   IP               NODE    NOMINATED NODE   READINESS GATES
coredns-5644d7b6d9-2msqz        1/1     Running   0          43h   10.244.0.2       node1   <none>           <none>
coredns-5644d7b6d9-84bzp        1/1     Running   0          43h   10.244.0.3       node1   <none>           <none>
etcd-node1                      1/1     Running   0          43h   140.82.8.255     node1   <none>           <none>
kube-apiserver-node1            1/1     Running   0          43h   140.82.8.255     node1   <none>           <none>
kube-controller-manager-node1   1/1     Running   0          43h   140.82.8.255     node1   <none>           <none>
kube-flannel-ds-amd64-2p8px     1/1     Running   0          42h   45.76.2.114      node2   <none>           <none>
kube-flannel-ds-amd64-65s4p     1/1     Running   0          42h   149.28.51.186    node4   <none>           <none>
kube-flannel-ds-amd64-bd7g8     1/1     Running   0          42h   140.82.8.255     node1   <none>           <none>
kube-flannel-ds-amd64-rqr46     1/1     Running   0          42h   108.61.128.115   node3   <none>           <none>
kube-proxy-2tvd9                1/1     Running   0          42h   149.28.51.186    node4   <none>           <none>
kube-proxy-6schm                1/1     Running   0          42h   108.61.128.115   node3   <none>           <none>
kube-proxy-fczl9                1/1     Running   0          43h   140.82.8.255     node1   <none>           <none>
kube-proxy-mzh7m                1/1     Running   0          42h   45.76.2.114      node2   <none>           <none>
kube-scheduler-node1            1/1     Running   0          43h   140.82.8.255     node1   <none>           <none>
```
上述输出可以看出，分别有3个flannel和proxy组件在运行，其中一个时master的，剩余两个是node节点的。

为了让kubectl在其他node节点上与您的集群通信，您需要像这样将管理员kubeconfig文件从您的主机复制到您的node节点主机上：
```
scp root@<master ip>:/etc/kubernetes/admin.conf .
在node节点上测试
kubectl --kubeconfig ./admin.conf get nodes
```
# 安装Dashboard控制面板管理界面


在master节点上执行下面语句
```
#下载资源配置清单
wget https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml
#部署资源
kubectl apply -f ./kubernetes-dashboard.yaml
kubectl apply -f https://github.com/kubernetes/dashboard/blob/master/aio/deploy/recommended/06_dashboard-deployment.yaml


https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
#测试
kubectl get pods -n kube-system

#发布端口
kubectl patch svc kubernetes-dashboard -p '{"spec":{"type":"NodePort"}}' -n kube-system
#查看服务端口
kubectl get svc -n kube-system 
#输出结果
NAME                   TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                  AGE
kube-dns               ClusterIP   10.96.0.10     <none>        53/UDP,53/TCP,9153/TCP   46h
kubernetes-dashboard   NodePort    10.107.60.27   <none>        443:32299/TCP            3m12s
```
使用token登录
```
#创建dashboard管理用户
kubectl create serviceaccount dashboard-admin -n kube-system
#绑定用户为集群管理用户 
kubectl create clusterrolebinding dashboard-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin
#查询生成的tocken
kubectl describe secret -n kube-system dashboard-admin-token
```
#测试：在浏览器测试：**注意是https**,(建议使用火狐浏览器在高级选项中许可访问)在浏览器输入https://集群任意IP:端口号
使用token令牌访问，复制刚才获取到的token，粘贴到网页中即可打开Dashboard管理界面。
```
#浏览器
https://45.76.2.114:32299/
登录后选择token登录，复制刚才产生的toke到浏览器即可
```
如果出现如下错误：the server could not find the requested resource,见报错

```
kubectl apply -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/grafana.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/influxdb.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/heapster.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/rbac/heapster-rbac.yaml
```

## 其他
```
#删除deployment
kubectl delete  deployment kubernetes-dashboard  -n kube-system
#删除service
kubectl delete  service  kubernetes-dashboard  -n kube-system
```

# 报错处理

报错1：

https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64/repodata/repomd.xml: [Errno 14] curl#7 - "Failed to connect to 2404:6800:4005:809::200e: Network is unreachable"

经测试，虚拟机不能访问外网，我需要实现虚拟机翻墙！

- [VM虚拟机使用主机shadowsocks代理上网](https://blog.csdn.net/zzl1243976730/article/details/63695131)
- [CENTOS使用PRIVOXY上网设置SHADOWSOCKS代理的方法](http://www.junww.com/server/2017/0302/221.html)
- [我的有道云笔记](http://note.youdao.com/noteshare?id=ff37a7893e334de8626663de604a3752&sub=E576C444FBDA4FF8AB06569062666DD6)

报错2：

查看版本是否匹配:
```
[root@kubernetes-master ~]# kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"14", GitVersion:"v1.16.1 ", GitCommit:"b7394102d6ef778017f2ca4046abbaa23b88c290", GitTreeState:"clean", BuildDate:"2019-04-08T17:08:49Z", GoVersion:"go1.12.1", Compiler:"gc", Platform:"linux/amd64"}
```
- [[ERROR ImagePull]: failed to pull image [k8s.gcr.io/kube-apiserver-amd64:v1.11.1]: exit status 1](http://www.bubuko.com/infodetail-2790243.html)

```
[ERROR ImagePull]: failed to pull image k8s.gcr.io/kube-apiserver:v1.16.1 : output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
, error: exit status 1
```
解决办法：

这是由于k8s.gcr.io国内无法连接上，先使用手动下载解决这个问题吧！
- [k8s.gcr.io国内无法连接解决方法](http://www.zhoufengjie.cn/?p=391)
```
docker pull mirrorgooglecontainers/kube-apiserver-amd64:v1.16.1 
docker pull mirrorgooglecontainers/kube-controller-manager-amd64:v1.16.1 
docker pull mirrorgooglecontainers/kube-scheduler-amd64:v1.16.1 
docker pull mirrorgooglecontainers/kube-proxy-amd64:v1.16.1 
docker pull mirrorgooglecontainers/pause:3.1
docker pull mirrorgooglecontainers/etcd-amd64:3.3.10
docker pull coredns/coredns:1.3.1
```
然后使用docker images 命令查看下载下来的镜像。
```
[root@kubernetes-master ~]# docker images 
REPOSITORY                                             TAG                 IMAGE ID            CREATED             SIZE
mirrorgooglecontainers/kube-proxy-amd64                v1.16.1             20a2d7035165        4 weeks ago         82.1MB
mirrorgooglecontainers/kube-apiserver-amd64            v1.16.1             cfaa4ad74c37        4 weeks ago         210MB
mirrorgooglecontainers/kube-controller-manager-amd64   v1.16.1             efb3887b411d        4 weeks ago         158MB
mirrorgooglecontainers/kube-scheduler-amd64            v1.16.1             8931473d5bdb        4 weeks ago         81.6MB
coredns/coredns                                        1.3.1               eb516548c180        3 months ago        40.3MB
mirrorgooglecontainers/etcd-amd64                      3.3.10              2c4adeb21b4f        5 months ago        258MB
mirrorgooglecontainers/pause                           3.1                 da86e6ba6ca1        16 months ago       742kB
```
我们还需要修改镜像的标签：
```
docker tag mirrorgooglecontainers/kube-apiserver-amd64:v1.16.1 k8s.gcr.io/kube-apiserver:v1.16.1 
docker tag mirrorgooglecontainers/kube-controller-manager-amd64:v1.16.1 k8s.gcr.io/kube-controller-manager:v1.16.1 
docker tag mirrorgooglecontainers/kube-scheduler-amd64:v1.16.1 k8s.gcr.io/kube-scheduler:v1.16.1 
docker tag mirrorgooglecontainers/kube-proxy-amd64:v1.16.1 k8s.gcr.io/kube-proxy:v1.16.1 
docker tag mirrorgooglecontainers/pause:3.1 k8s.gcr.io/pause:3.1
docker tag mirrorgooglecontainers/etcd-amd64:3.3.10 k8s.gcr.io/etcd:3.3.10
docker tag coredns/coredns:1.3.1 k8s.gcr.io/coredns:1.3.1
```
删除原来的镜像 
```
docker rmi mirrorgooglecontainers/kube-apiserver-amd64:v1.16.1 
docker rmi mirrorgooglecontainers/kube-controller-manager-amd64:v1.16.1 
docker rmi mirrorgooglecontainers/kube-scheduler-amd64:v1.16.1 
docker rmi mirrorgooglecontainers/kube-proxy-amd64:v1.16.1 
docker rmi mirrorgooglecontainers/pause:3.1
docker rmi mirrorgooglecontainers/etcd-amd64:3.3.10
docker rmi coredns/coredns:1.3.1
```
然后使用docker images 命令查看下载下来的镜像。
```
[root@kubernetes-master ~]# docker images
REPOSITORY                           TAG                 IMAGE ID            CREATED             SIZE
k8s.gcr.io/kube-proxy                v1.16.1             20a2d7035165        5 weeks ago         82.1MB
k8s.gcr.io/kube-apiserver            v1.16.1             cfaa4ad74c37        5 weeks ago         210MB
k8s.gcr.io/kube-scheduler            v1.16.1             8931473d5bdb        5 weeks ago         81.6MB
k8s.gcr.io/kube-controller-manager   v1.16.1             efb3887b411d        5 weeks ago         158MB
k8s.gcr.io/coredns                   1.3.1               eb516548c180        4 months ago        40.3MB
k8s.gcr.io/etcd                      3.3.10              2c4adeb21b4f        5 months ago        258MB
k8s.gcr.io/pause                     3.1                 da86e6ba6ca1        16 months ago       742kB
```

我觉得还可以设置docker代理来解决这个问题！

- [K8S 报 ErrImagePull k8s.gcr.io国内无法连接解决方法](https://blog.csdn.net/h952520296/article/details/86981343)

报错3：
```
unable to recognize "https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml": Get http://localhost:8080/api?timeout=32s: dial tcp [::1]:8080: connect: connection refused
```
```
systemctl restart kubelet
```
- [k8s V1.11.1报错：The connection to the server localhost:8080 was refused](https://blog.csdn.net/I_will_try/article/details/89946042)

报错4：
[ERROR Port-10250]: Port 10250 is in use 解决办法：
```
kubeadm reset
```
- [初始化 Kubernetes 问题（端口占用）](https://blog.csdn.net/u013004700/article/details/81326706)

报错5：
```
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get http://localhost:10248/healthz: dial tcp [::1]:10248: connect: connection refused.
[kubelet-check] It seems like the kubelet isn't running or healthy.
```
可能是manifast已经存在，删除即可：
```
rm -rf /etc/kubernetes/manifests
```
- [It seems like the kubelet isn't running or healthy.](https://segmentfault.com/a/1190000011764684)

报错6：
```
[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
```
是不是没有启动kubelet服务，使能服务如下：
```
systemctl enable kubelet.service
```

报错7：

在执行命令kubeadm join 命令时报错：
```
running with swap on is not supported. Please disable swap
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
```
网络上的说如下，[网址](https://www.jianshu.com/p/70efa1b853f5)： 
```
swapoff -a
```
[ERROR]的话就要重视，虽然可以通过--ignore-preflight-errors忽略错误强制安装，但为了不出各种奇怪的毛病，所以强烈建议error的问题
一定要解决了再继续执行下去。比如系统资源不满足要求（master节点要求至少2C2G），swap没关等等（会影响kubelet的启动），swap的话可以通过设置swapoff
-a来进行关闭，另外注意/proc/sys/net/bridge/bridge-nf-call-iptables这个参数，需要设置为1，否则kubeadm预检也会通不过，貌似网络插件会用到这个内核参数。

报错7：

参考网址：

- [kubernetes nodes notready 解决思路](https://blog.csdn.net/qq_21816375/article/details/80222689)

一般性解决办法是在该node节点上执行命令：
```
//重启docker
systemctl restart docker
```

报错8： 

- [docker pull下载镜像报错Get https://registry-1.docker.io/v2/library/centos/manifests/latest:..... timeout](https://www.cnblogs.com/kevingrace/p/9882026.html)

报错9：

- [使用io.fabric8 client API 创建部署报错：“the server could not find the requested resource”](https://blog.csdn.net/hongweigg/article/details/80584695)

在使用部署文件进行部署时，发现总是报“the server could not find the requested resource”错误，后来发现原来是部署文件中声明的 apiVersion头的问题，可以
查看dashboard日志：Metric client health check failed: the server could not find the requested resource (get services heapster). Retrying in 30 seconds.，这是没有heapter的原因


```
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: metrics-server
  namespace: kube-system
  labels:
    k8s-app: metrics-server
spec:
  selector:
    matchLabels:
      k8s-app: metrics-server
  template:
    metadata:
      name: metrics-server
      labels:
        k8s-app: metrics-server
    spec:
      serviceAccountName: metrics-server
      volumes:
      # mount in tmp so we can safely use from-scratch images and/or read-only containers
      - name: tmp-dir
        emptyDir: {}
      containers:
      - name: metrics-server
        image: k8s.gcr.io/metrics-server-amd64:v0.3.6
        imagePullPolicy: Always
      command:
        - /metrics-server
        - --kubelet-insecure-tls
        - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP
        volumeMounts:
        - name: tmp-dir
          mountPath: /tmp
```

curl -k -H "Content-Type: application/json" -X PUT --data-binary @org1-example-com.json http://127.0.0.1:8001/api/v1/namespaces/org1-example-com/finalize